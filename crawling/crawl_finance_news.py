import os
from datetime import datetime
from typing import List, Dict

import pymysql
import feedparser
from dotenv import load_dotenv

# ğŸ”§ .env ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ENV_PATH = os.path.join(BASE_DIR, ".env")
load_dotenv(ENV_PATH)

# ğŸ”§ MySQL ì„¤ì •: .envì—ì„œ ì½ì–´ì˜¤ê¸°
DB_CONFIG = {
    "host": os.getenv("MYSQL_HOST", "localhost"),
    "port": int(os.getenv("MYSQL_PORT", "3306")),
    "user": os.getenv("MYSQL_USER"),
    "password": os.getenv("MYSQL_PASSWORD"),
    "database": os.getenv("MYSQL_DATABASE"),
    "charset": "utf8mb4",
}

# ğŸ”§ ê²½ì œ ë‰´ìŠ¤ RSS (ì˜ˆì‹œ: ì—°í•©ë‰´ìŠ¤TV ê²½ì œ)
ECON_RSS_URL = "http://www.yonhapnewstv.co.kr/category/news/economy/feed/"


def crawl_economy_news(limit: int = 20) -> List[Dict]:
    """
    ê²½ì œ ë‰´ìŠ¤ RSSì—ì„œ ìµœì‹  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
    ë°˜í™˜: [{title, url, published_at, source_type, summary}, ...]
    """
    feed = feedparser.parse(ECON_RSS_URL)
    articles: List[Dict] = []

    for entry in feed.entries[:limit]:
        title = (entry.get("title") or "").strip()
        url = (entry.get("link") or "").strip()

        # ë‚ ì§œ ì •ë³´ (published / updated ì¤‘ í•˜ë‚˜ ì‚¬ìš©)
        published = entry.get("published_parsed") or entry.get("updated_parsed")
        if published:
            published_at = datetime(
                published.tm_year,
                published.tm_mon,
                published.tm_mday,
                published.tm_hour,
                published.tm_min,
                published.tm_sec,
            )
        else:
            published_at = None

        summary = entry.get("summary", "")

        articles.append(
            {
                "title": title,
                "url": url,
                "published_at": published_at,
                "source_type": "yonhap_economy_rss",
                "summary": summary,
            }
        )

    return articles


def insert_articles(articles: List[Dict]):
    """
    í¬ë¡¤ë§í•œ ê¸°ì‚¬ë“¤ì„ MySQL articles í…Œì´ë¸”ì— ì €ì¥
    url ì»¬ëŸ¼ì— UNIQUE ì¸ë±ìŠ¤ê°€ ê±¸ë ¤ ìˆë‹¤ê³  ê°€ì • (ì¤‘ë³µ ë°©ì§€)
    """
    if not articles:
        print("âŒ ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    conn = pymysql.connect(**DB_CONFIG)
    try:
        with conn.cursor() as cur:
            sql = """
            INSERT INTO articles (title, url, published_at, source_type, summary)
            VALUES (%s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                title = VALUES(title),
                published_at = VALUES(published_at),
                source_type = VALUES(source_type),
                summary = VALUES(summary);
            """
            for a in articles:
                cur.execute(
                    sql,
                    (
                        a["title"],
                        a["url"],
                        a["published_at"],
                        a["source_type"],
                        a["summary"],
                    ),
                )
        conn.commit()
        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ")
    finally:
        conn.close()


if __name__ == "__main__":
    print("ğŸ“¡ ê²½ì œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    news_list = crawl_economy_news(limit=20)

    for n in news_list:
        print(f"- {n['published_at']} | {n['title']}")

    print("\nğŸ’¾ DB ì €ì¥ ì‹œì‘...")
    insert_articles(news_list)
    print("ğŸ‰ ì‘ì—… ì™„ë£Œ!")
